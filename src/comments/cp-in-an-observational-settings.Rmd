---
title: "CP in an Observational Setting"
author: "Ilya V. Schurov"
date: "12/10/2017"
output: html_document
---

This is a formalization of a diagram posted by William Fedus.

**Input**: a sequence of multidimensional vectors $x_t$, each $x_t \in \mathbb R^N$, $t\in \mathbb N$ (or $\mathbb Z$), $N$ may be quite large (e.g. pixels of a video frame)

**Representation state** at the moment $t$ is $h_t \in \mathbb R^r$, $r$ is a dimension of the representation space (may be quite large). We denote $k$-th component of this vector as $h_t[k]$. Let us denote a set $\{1, \ldots, r\}$ by $\mathbb N_r$. If $K=(k_1,\ldots, k_s) \in \mathbb N_r^s$ is a vector of indexes, we denote the vector of the corresponding values $(h_t[k_1],\ldots, h_t[k_n]) \in \mathbb R^s$ by $h_t[K]$.

**Conscious state** at the moment $t$ is $c_t=(B_t, b_t, A_t)$[^1], where $B_t\in \mathbb N_r^s$ is a vector of indexes of $h_t$ we are interested in at a particular moment, $b_t\in \mathbb R^s$ is the corresponding values, i.e. $b_t=h_t[B_t]$ and $A_t=(A_{1, t}, \ldots, A_{p, t})\in \mathbb N_r^p$ is a vector of indexes we are going to predict. (Should we demand $p=s$, i.e. predictions have the same dimension as current conscious state?)

[^1]: This notation is a bit different from William's but consistent with Bengio's paper: $c_t$ is a full consious state, including $A_t$. In William's diagram $c_t=(B_t, b_t)$.

**Representation network** $R$ is a function (presented by RNN):
$$
h_t=R(x_t, h_{t-1}).
$$
(In Bengio's paper the same thing is called $F$.)

**Consiousness network** is a function (presented by another RNN):
$$
c_{t} = C(h_t, c_{t-1}, z_t),
$$
where $z_t$ is a random noise source. Actually, $C$ have to define only $B_t$ and $A_t$ parts of $c_t$ as $b_t$ is defined by the relation $b_t=h_t[B_t]$. So the domain of $C$ is $\mathbb N_r^s\times\mathbb N_r^p$.

**Generator network** is a function
$$
  \widehat{a}_t = G(c_t).
$$
The objective of the generator is to predict the value of the representation vector $h_{t+1}$ in the next moment at indexes $A_t$. So we have a **MSE loss function**:
$$
\mathcal L=\sum_{i=1}^{p}(\widehat{a}_{i, t}-h_{t+1}[A_{i, t}])^2
$$

As the objective of the consciouss is to be able to predict the part of the future representation vector, we also introduce **mutual information loss function**: we want to maximize the following mutual information:[^2]

$$I(b_t, h_{t+1}[A_t]) = H(h_{t+1}[A_t]) - H(h_{t+1}[A_t])\mid b_t)$$

[^2]: In William's diagram, we have $[A_{i, t}]$ instead of $[A_t]$, but I believe we are interested in a full vector of $A_t$ here and not only $i$'s component of it.