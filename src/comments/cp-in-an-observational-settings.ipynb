{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CP in an Observational Setting\n",
    "\n",
    "This is a formalization of a diagram posted by William Fedus.\n",
    "\n",
    "**Input**: a sequence of multidimensional vectors $x_t$, each $x_t \\in \\mathbb R^N$, $t\\in \\mathbb N$ (or $\\mathbb Z$), $N$ may be quite large (e.g. pixels of a video frame)\n",
    "\n",
    "**Representation state** at the moment $t$ is $h_t \\in \\mathbb R^r$, $r$ is a dimension of the representation space (may be quite large). We denote $k$-th component of this vector as $h_t[k]$. Let us denote a set $\\{1, \\ldots, r\\}$ by $\\mathbb N_r$. If $K=(k_1,\\ldots, k_s) \\in \\mathbb N_r^s$ is a vector of indexes, we denote the vector of the corresponding values $(h_t[k_1],\\ldots, h_t[k_n]) \\in \\mathbb R^s$ by $h_t[K]$.\n",
    "\n",
    "**Conscious state** at the moment $t$ is $c_t=(B_t, b_t, A_t)$[^1], where $B_t\\in \\mathbb N_r^s$ is a vector of indexes of $h_t$ we are interested in at a particular moment, $b_t\\in \\mathbb R^s$ is the corresponding values, i.e. $b_t=h_t[B_t]$ and $A_t=(A_{1, t}, \\ldots, A_{p, t})\\in \\mathbb N_r^p$ is a vector of indexes we are going to predict.\n",
    "\n",
    "[^1]: This notation is a bit different from William's but consistent with Bengio's paper: $c_t$ is a full consious state, including $A_t$. In William's diagram $c_t=(B_t, b_t)$.\n",
    "\n",
    "**Representation network** $R$ is a function (presented by RNN):\n",
    "$$\n",
    "h_t=R(x_t, h_{t-1}).\n",
    "$$\n",
    "(In Bengio's paper the same thing is called $F$.)\n",
    "\n",
    "**Consiousness network** is a function (presented by another RNN):\n",
    "$$\n",
    "c_{t} = C(h_t, c_{t-1}, z_t),\n",
    "$$\n",
    "where $z_t$ is a random noise source. Actually, $C$ have to define only $B_t$ and $A_t$ parts of $c_t$ as $b_t$ is defined by the relation $b_t=h_t[B_t]$. So the domain of $C$ is $\\mathbb N_r^s\\times\\mathbb N_r^p$.\n",
    "\n",
    "**Generator network** is a function\n",
    "$$\n",
    "  \\widehat{a}_t = G(c_t).\n",
    "$$\n",
    "The objective of the generator is to predict the value of the representation vector $h_{t+1}$ in the next moment at indexes $A_t$. So we have a **MSE loss function**:\n",
    "$$\n",
    "\\mathcal L=\\sum_{i=1}^{p}(\\widehat{a}_{i, t}-h_{t+1}[A_{i, t}])^2\n",
    "$$\n",
    "\n",
    "As the objective of the consciouss is to be able to predict the part of the future representation vector, we also introduce **mutual information loss function**: we want to maximize the following mutual information:[^2]\n",
    "\n",
    "$$I(b_t, h_{t+1}[A_t]) = H(h_{t+1}[A_t]) - H(h_{t+1}[A_t])\\mid b_t)$$\n",
    "\n",
    "[^2]: In William's diagram, we have $[A_{i, t}]$ instead of $[A_t]$, but I believe we are interested in a full vector of $A_t$ here and not only $i$'s component of it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
